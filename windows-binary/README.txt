ğŸš€ COLOSSUS CLI v1.1 - Windows Binary
======================================

This folder contains the Windows executable for Colossus CLI, a powerful
Go-based alternative to Ollama for running large language models locally.

âœ¨ NEW in v1.1:
- ğŸ“Š Visual progress bars for model downloads
- âš¡ Real-time download speed and ETA display
- ğŸš€ Enhanced GGUF model repositories with 7 guaranteed models
- ğŸ¤– REAL AI INFERENCE: Defaults to llama.cpp (no more simulation!)
- ğŸ”¥ Production-ready with actual model responses

ğŸ“ FILES:
----------
- colossus.exe        : Main executable
- run-colossus.bat    : Interactive menu launcher
- start-server.bat    : Quick server start
- README.txt          : This file

ğŸ¯ HOW TO USE:
--------------

Option 1: Double-click "run-colossus.bat" for an interactive menu

Option 2: Double-click "start-server.bat" to quickly start the API server

Option 3: Use Command Prompt/PowerShell:
   cd "path\to\this\folder"
   colossus.exe --help

ğŸš€ QUICK START:
---------------
1. Double-click "run-colossus.bat"
2. Choose option 1 to start the server
3. Choose option 5 to download a model (try "qwen" - smallest)
4. Choose option 6 to start chatting

ğŸ“¦ AVAILABLE GGUF MODELS:
-------------------------
- qwen      (0.5B) - Smallest, fastest, great for testing
- tinyllama (1.1B) - Popular lightweight model
- gemma     (2B)   - Google's Gemma model
- phi       (2.7B) - Microsoft's Phi model  
- mistral   (7B)   - Larger, more capable
- llama2    (7B)   - Meta's Llama-2
- codellama (7B)   - Specialized for coding tasks

ğŸŒ API USAGE:
-------------
Once the server is running, you can use it via HTTP:

# List models
curl http://localhost:11434/api/tags

# Generate text
curl -X POST http://localhost:11434/api/generate ^
  -H "Content-Type: application/json" ^
  -d "{\"model\": \"tinyllama\", \"prompt\": \"Hello world\"}"

# Chat
curl -X POST http://localhost:11434/api/chat ^
  -H "Content-Type: application/json" ^
  -d "{\"model\": \"tinyllama\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}"

âœ¨ FEATURES:
------------
- ğŸ”¥ Full Ollama API compatibility
- âš¡ Real inference with llama.cpp integration  
- ğŸš€ GPU acceleration (CUDA/ROCm/Metal)
- ğŸ¤— Hugging Face Hub integration
- ğŸ“¦ Automatic model management
- ğŸ›¡ï¸ Model format validation
- ğŸ’¬ Interactive chat sessions
- ğŸŒ REST API for integrations

ğŸ†˜ TROUBLESHOOTING:
-------------------
- If you see "This is a command line tool", use the .bat files or command prompt
- If server fails to start, check if port 11434 is available
- For GPU acceleration, ensure proper drivers are installed
- Models are downloaded to: %USERPROFILE%\.colossus\models

ğŸ“– MORE INFO:
-------------
GitHub: https://github.com/yourusername/colossus-cli
Documentation: https://yourusername.github.io/colossus-cli

Built with â¤ï¸ in Go
